{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "934a940b-8d62-434b-8f64-24f21d299128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall numpy pandas scikit-surprise scipy -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34f9bcf0-19ef-4dc2-8599-f844885cf85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy==1.26.4 pandas scikit-surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3865880a-a0ad-4164-885f-e02ca940a7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re \n",
    "\n",
    "from surprise import Reader, Dataset, SVD, BaselineOnly # Import BaselineOnly\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import accuracy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e0032aa-325e-4fd1-a93c-72fb2806d076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ratings.csv ...\n",
      "Loaded ratings.csv: shape=(25000095, 4)\n",
      "Loading movies.csv ...\n",
      "Loaded movies.csv: shape=(62423, 3)\n",
      "Loading tags.csv ...\n",
      "Loaded tags.csv: shape=(1093360, 4)\n",
      "Loading links.csv ...\n",
      "Loaded links.csv: shape=(62423, 3)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Path to extracted MovieLens dataset\n",
    "DATA_DIR = './data/ml-25m'\n",
    "\n",
    "# Optimized dtypes\n",
    "rating_dtypes = {\n",
    "    'userId': 'int32',\n",
    "    'movieId': 'int32',\n",
    "    'rating': 'float32',\n",
    "    'timestamp': 'int32'\n",
    "}\n",
    "movie_dtypes = {\n",
    "    'movieId': 'int32',\n",
    "    'title': 'string',\n",
    "    'genres': 'string'\n",
    "}\n",
    "tag_dtypes = {\n",
    "    'userId': 'int32',\n",
    "    'movieId': 'int32',\n",
    "    'tag': 'string',\n",
    "    'timestamp': 'int32'\n",
    "}\n",
    "link_dtypes = {\n",
    "    'movieId': 'int32',\n",
    "    'imdbId': 'float32',\n",
    "    'tmdbId': 'float32'\n",
    "}\n",
    "\n",
    "\n",
    "def load_csv(filename, dtypes):\n",
    "    path = os.path.join(DATA_DIR, filename)\n",
    "    print(f\"Loading {filename} ...\")\n",
    "    df = pd.read_csv(path, dtype=dtypes)\n",
    "    print(f\"Loaded {filename}: shape={df.shape}\")\n",
    "    return df\n",
    "\n",
    "ratings = load_csv('ratings.csv', rating_dtypes)\n",
    "movies  = load_csv('movies.csv', movie_dtypes)\n",
    "tags    = load_csv('tags.csv', tag_dtypes)\n",
    "links   = load_csv('links.csv', link_dtypes)\n",
    "# genome_tags = load_csv('genome-tags.csv')\n",
    "# genome_scores = pd.read_csv('./data/ml-25m/genome-scores.csv-scores.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "079a3474-8138-4352-b922-5f75dd0bb5e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1147880044</td>\n",
       "      <td>Pulp Fiction (1994)</td>\n",
       "      <td>Comedy|Crime|Drama|Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>306</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1147868817</td>\n",
       "      <td>Three Colors: Red (Trois couleurs: Rouge) (1994)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>307</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1147868828</td>\n",
       "      <td>Three Colors: Blue (Trois couleurs: Bleu) (1993)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>665</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1147878820</td>\n",
       "      <td>Underground (1995)</td>\n",
       "      <td>Comedy|Drama|War</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>899</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1147868510</td>\n",
       "      <td>Singin' in the Rain (1952)</td>\n",
       "      <td>Comedy|Musical|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25000090</th>\n",
       "      <td>162541</td>\n",
       "      <td>50872</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1240953372</td>\n",
       "      <td>Ratatouille (2007)</td>\n",
       "      <td>Animation|Children|Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25000091</th>\n",
       "      <td>162541</td>\n",
       "      <td>55768</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1240951998</td>\n",
       "      <td>Bee Movie (2007)</td>\n",
       "      <td>Animation|Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25000092</th>\n",
       "      <td>162541</td>\n",
       "      <td>56176</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1240950697</td>\n",
       "      <td>Alvin and the Chipmunks (2007)</td>\n",
       "      <td>Children|Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25000093</th>\n",
       "      <td>162541</td>\n",
       "      <td>58559</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1240953434</td>\n",
       "      <td>Dark Knight, The (2008)</td>\n",
       "      <td>Action|Crime|Drama|IMAX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25000094</th>\n",
       "      <td>162541</td>\n",
       "      <td>63876</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1240952515</td>\n",
       "      <td>Milk (2008)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000095 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          userId  movieId  rating   timestamp  \\\n",
       "0              1      296     5.0  1147880044   \n",
       "1              1      306     3.5  1147868817   \n",
       "2              1      307     5.0  1147868828   \n",
       "3              1      665     5.0  1147878820   \n",
       "4              1      899     3.5  1147868510   \n",
       "...          ...      ...     ...         ...   \n",
       "25000090  162541    50872     4.5  1240953372   \n",
       "25000091  162541    55768     2.5  1240951998   \n",
       "25000092  162541    56176     2.0  1240950697   \n",
       "25000093  162541    58559     4.0  1240953434   \n",
       "25000094  162541    63876     5.0  1240952515   \n",
       "\n",
       "                                                     title  \\\n",
       "0                                      Pulp Fiction (1994)   \n",
       "1         Three Colors: Red (Trois couleurs: Rouge) (1994)   \n",
       "2         Three Colors: Blue (Trois couleurs: Bleu) (1993)   \n",
       "3                                       Underground (1995)   \n",
       "4                               Singin' in the Rain (1952)   \n",
       "...                                                    ...   \n",
       "25000090                                Ratatouille (2007)   \n",
       "25000091                                  Bee Movie (2007)   \n",
       "25000092                    Alvin and the Chipmunks (2007)   \n",
       "25000093                           Dark Knight, The (2008)   \n",
       "25000094                                       Milk (2008)   \n",
       "\n",
       "                               genres  \n",
       "0         Comedy|Crime|Drama|Thriller  \n",
       "1                               Drama  \n",
       "2                               Drama  \n",
       "3                    Comedy|Drama|War  \n",
       "4              Comedy|Musical|Romance  \n",
       "...                               ...  \n",
       "25000090     Animation|Children|Drama  \n",
       "25000091             Animation|Comedy  \n",
       "25000092              Children|Comedy  \n",
       "25000093      Action|Crime|Drama|IMAX  \n",
       "25000094                        Drama  \n",
       "\n",
       "[25000095 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pd.merge(ratings,movies)\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98f32211-9ece-4076-8767-b99ce4e7692b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- BASELINE MODEL RESULTS ---\n",
      "1. Global Mean (μ): 3.5339\n",
      "\n",
      "2. User Biases (b_u): The User's Leniency/Harshness\n",
      "        userId  user_bias\n",
      "0            1   0.009460\n",
      "1            2  -0.025164\n",
      "2            3   0.244178\n",
      "3            4  -0.262870\n",
      "4            5   0.271491\n",
      "...        ...        ...\n",
      "162536  162537   0.712264\n",
      "162537  162538  -0.202443\n",
      "162538  162539   0.736999\n",
      "162539  162540   0.497151\n",
      "162540  162541  -0.188255\n",
      "\n",
      "[162541 rows x 2 columns]\n",
      "\n",
      "3. Item Biases (b_i): The Movie's Popularity/Quality\n",
      "       movieId  item_bias\n",
      "29523   136782   1.466145\n",
      "49654   186119   1.466145\n",
      "29643   137032   1.466145\n",
      "49041   184643   1.466145\n",
      "29646   137038   1.466145\n",
      "...        ...        ...\n",
      "5693      5805  -3.033855\n",
      "55757   199922  -3.033855\n",
      "53387   194608  -3.033855\n",
      "58517   207153  -3.033855\n",
      "45900   177419  -3.033855\n",
      "\n",
      "[59047 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- BASELINE MODEL CALCULATIONS ---\n",
    "\n",
    "def calculate_baseline_biases(data: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Calculates the Global Mean, User Bias, and Item Bias.\n",
    "    These three components form the complete baseline model prediction.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Global Mean (mu)\n",
    "    # The simplest possible prediction for any unknown rating\n",
    "    global_mean = data['rating'].mean()\n",
    "\n",
    "    # 2. Item Bias (b_i)\n",
    "    # Average deviation of each movie's ratings from the global mean\n",
    "    \n",
    "    # Calculate Mean Rating for each Movie\n",
    "    movie_mean = data.groupby('movieId')['rating'].mean().reset_index(name='movie_mean')\n",
    "    \n",
    "    # Calculate Item Bias\n",
    "    movie_mean['item_bias'] = movie_mean['movie_mean'] - global_mean\n",
    "    item_bias_df = movie_mean[['movieId', 'item_bias']]\n",
    "\n",
    "    # 3. User Bias (b_u)\n",
    "    # Average deviation of each user's ratings from the global mean (after removing item effect)\n",
    "    \n",
    "    # Merge Item Bias back into the main data to calculate adjusted user ratings\n",
    "    data_with_bias = pd.merge(data, item_bias_df, on='movieId', how='left')\n",
    "    \n",
    "    # Adjusted Rating = Original Rating - (Global Mean + Item Bias)\n",
    "    # Adjusted Rating should ideally only contain the User Bias (plus noise)\n",
    "    data_with_bias['adjusted_rating'] = data_with_bias['rating'] - (global_mean + data_with_bias['item_bias'])\n",
    "    \n",
    "    # Calculate User Bias (mean of the adjusted ratings for each user)\n",
    "    user_bias_df = data_with_bias.groupby('userId')['adjusted_rating'].mean().reset_index(name='user_bias')\n",
    "\n",
    "    return global_mean, item_bias_df, user_bias_df\n",
    "\n",
    "def baseline_predict(global_mean, user_id, movie_id, item_bias_df, user_bias_df):\n",
    "    \"\"\"\n",
    "    Predicts a rating using the baseline formula: mu + b_u + b_i\n",
    "    \"\"\"\n",
    "    # 1. Get Global Mean (mu)\n",
    "    mu = global_mean\n",
    "\n",
    "    # 2. Get Item Bias (b_i)\n",
    "    b_i_series = item_bias_df[item_bias_df['movieId'] == movie_id]['item_bias']\n",
    "    b_i = b_i_series.iloc[0] if not b_i_series.empty else 0.0\n",
    "\n",
    "    # 3. Get User Bias (b_u)\n",
    "    b_u_series = user_bias_df[user_bias_df['userId'] == user_id]['user_bias']\n",
    "    b_u = b_u_series.iloc[0] if not b_u_series.empty else 0.0\n",
    "    \n",
    "    # Baseline Prediction Formula:\n",
    "    predicted_rating = mu + b_u + b_i\n",
    "    \n",
    "    return predicted_rating, mu, b_u, b_i\n",
    "\n",
    "\n",
    "# 1. Calculate the Biases\n",
    "global_mean, item_bias_df, user_bias_df = calculate_baseline_biases(data)\n",
    "\n",
    "if global_mean is not None:\n",
    "    print(\"\\n--- BASELINE MODEL RESULTS ---\")\n",
    "    print(f\"1. Global Mean (\\u03bc): {global_mean:.4f}\")\n",
    "\n",
    "    print(\"\\n2. User Biases (b_u): The User's Leniency/Harshness\")\n",
    "    print(user_bias_df)\n",
    "\n",
    "    print(\"\\n3. Item Biases (b_i): The Movie's Popularity/Quality\")\n",
    "    print(item_bias_df.sort_values('item_bias', ascending=False))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e679f60-c40b-461d-a27d-62f530073fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- BASELINE PREDICTION EXAMPLE (User 1 for Movie 899) ---\n",
      "Actual Rating: 3.50\n",
      "Prediction Components:\n",
      "  μ (Global Mean): 3.5339\n",
      "  b_u (User Bias):   0.0095\n",
      "  b_i (Item Bias):   0.5171\n",
      "  Baseline Prediction (μ + b_u + b_i): 4.0604\n"
     ]
    }
   ],
   "source": [
    "# 4. Example Prediction\n",
    "    \n",
    "user_to_predict = 1\n",
    "movie_to_predict = 899\n",
    "\n",
    "predicted_rating, mu, b_u, b_i = baseline_predict(\n",
    "    global_mean, user_to_predict, movie_to_predict, item_bias_df, user_bias_df\n",
    ")\n",
    "\n",
    "actual_rating = data[(data['userId'] == user_to_predict) & (data['movieId'] == movie_to_predict)]['rating'].iloc[0]\n",
    "\n",
    "print(f\"\\n--- BASELINE PREDICTION EXAMPLE (User {user_to_predict} for Movie {movie_to_predict}) ---\")\n",
    "print(f\"Actual Rating: {actual_rating:.2f}\")\n",
    "print(f\"Prediction Components:\")\n",
    "print(f\"  \\u03bc (Global Mean): {mu:.4f}\")\n",
    "print(f\"  b_u (User Bias):   {b_u:.4f}\")\n",
    "print(f\"  b_i (Item Bias):   {b_i:.4f}\")\n",
    "print(f\"  Baseline Prediction (\\u03bc + b_u + b_i): {predicted_rating:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f4780f7-8565-430d-82c4-ae44f5831844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 4. Training Matrix Factorization (SVD) Model ---\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Data Preparation for Surprise ---\n",
    "\n",
    "reader = Reader(rating_scale=(0.5, 5.0))\n",
    "\n",
    "# Load the DataFrame into the Surprise Dataset format\n",
    "data = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)\n",
    "\n",
    "# --- 2. Train-Test Split ---\n",
    "# Split data into 80% training set and 20% test set\n",
    "trainset, testset = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- 3. Matrix Factorization (SVD Implementation) ---\n",
    "\n",
    "# SVD is the implementation of Matrix Factorization trained via SGD\n",
    "algo = SVD(\n",
    "    n_factors=100,  # Number of latent factors (k)\n",
    "    n_epochs=20,    # Number of training iterations\n",
    "    lr_all=0.005,   # Learning rate\n",
    "    reg_all=0.02,   # Regularization term\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"\\n--- 4. Training Matrix Factorization (SVD) Model ---\")\n",
    "# Train the algorithm on the training set\n",
    "algo.fit(trainset)\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a37658e9-1e99-4b19-9d7c-1b6c4181d411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 5. Evaluation Results ---\n",
      "SVD RMSE (Root Mean Squared Error): 0.7773\n",
      "SVD MAE (Mean Absolute Error): 0.5865\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 5. Evaluation ---\n",
    "\n",
    "# Predict ratings for the test set\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# Calculate RMSE (Root Mean Squared Error) and MAE\n",
    "rmse = accuracy.rmse(predictions, verbose=False)\n",
    "mae = accuracy.mae(predictions, verbose=False)\n",
    "\n",
    "print(\"\\n--- 5. Evaluation Results ---\")\n",
    "print(f\"SVD RMSE (Root Mean Squared Error): {rmse:.4f}\")\n",
    "print(f\"SVD MAE (Mean Absolute Error): {mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c94a5c5-a551-4ecd-b227-fa17d16a088c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total test set predictions generated: 5,000,019\n",
      "\n",
      "--- 6. Sample of 10 Predictions from Test Set ---\n",
      "This shows where the model's prediction (Predicted_Rating) matched the user's actual rating.\n",
      "         userId  movieId  Actual_Rating  Predicted_Rating\n",
      "547077    70745     3507            3.5          3.352319\n",
      "260763     2076     3133            4.0          3.618980\n",
      "2148015   10397    69844            3.0          2.792635\n",
      "3627539   92953     3624            3.0          3.094113\n",
      "4692539   81695     6539            3.5          3.555909\n",
      "4717853   33960      986            4.0          3.422375\n",
      "967463   108059     1663            1.5          2.892461\n",
      "3240406    9015     1258            4.5          4.602095\n",
      "3217118   87138     8961            3.0          3.838990\n",
      "1206106   35316     1389            2.0          2.189074\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 6. Get Sample of Predictions ---\n",
    "\n",
    "def get_prediction_sample(predictions, n_samples=5):\n",
    "    \"\"\"Converts Surprise predictions into a DataFrame and returns a sample.\"\"\"\n",
    "    # Convert predictions (named tuple list) to a dictionary list\n",
    "    pred_data = [\n",
    "        (uid, iid, r_ui, est, details['was_impossible'])\n",
    "        for (uid, iid, r_ui, est, details) in predictions\n",
    "    ]\n",
    "    \n",
    "    # Create the DataFrame\n",
    "    pred_df = pd.DataFrame(\n",
    "        pred_data,\n",
    "        columns=['userId', 'movieId', 'Actual_Rating', 'Predicted_Rating', 'Impossible']\n",
    "    )\n",
    "    \n",
    "    # Merge with movie titles (assuming the global movies_df is available, though \n",
    "    # we'll use a placeholder here for independence)\n",
    "    \n",
    "    # NOTE: You will need to load the 'movies.csv' file earlier in your full script \n",
    "    # and use it here to map movieIds to titles for better interpretation.\n",
    "    \n",
    "    print(f\"Total test set predictions generated: {len(pred_df):,}\")\n",
    "    return pred_df.sample(n=n_samples, random_state=42)\n",
    "    \n",
    "N_SAMPLE = 10\n",
    "prediction_sample_df = get_prediction_sample(predictions, N_SAMPLE)\n",
    "\n",
    "print(f\"\\n--- 6. Sample of {N_SAMPLE} Predictions from Test Set ---\")\n",
    "print(\"This shows where the model's prediction (Predicted_Rating) matched the user's actual rating.\")\n",
    "print(prediction_sample_df[['userId', 'movieId', 'Actual_Rating', 'Predicted_Rating']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54c731e2-44f3-4f50-8900-80ea24b26baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 6. Example Prediction for User 1 on Movie 899 ---\n",
      "Predicted rating: 3.5558\n",
      "\n",
      "--- 7. Learned Biases from SVD ---\n",
      "Learned Global Mean (μ): 3.5336\n",
      "Learned User Bias (b_u):   -0.1323\n",
      "Learned Item Bias (b_i):   0.3434\n",
      "Learned Interaction Term:  -0.1889\n",
      "\n",
      "Prediction Check: μ + b_u + b_i + interaction_term = 3.5558\n"
     ]
    }
   ],
   "source": [
    "# --- 7. Accessing Learned Biases (for comparison to your EDA) ---\n",
    "\n",
    "test_user_id = 1\n",
    "test_movie_id = 899\n",
    "\n",
    "def get_learned_biases(algo, test_user_id, test_movie_id):\n",
    "    \"\"\"Retrieves the learned user and item bias values from the SVD model.\"\"\"\n",
    "    try:\n",
    "        # Get the internal ID (the index) for the movie and user from the trained model\n",
    "        # This is necessary because the bias arrays are indexed by internal ID, not raw MovieLens ID\n",
    "        internal_movie_id = algo.trainset.to_inner_iid(test_movie_id)\n",
    "        internal_user_id = algo.trainset.to_inner_uid(test_user_id)\n",
    "\n",
    "        # Access the bias arrays directly (bu for user bias, bi for item bias)\n",
    "        # These are NumPy arrays indexed by the internal ID\n",
    "        item_bias = algo.bi[internal_movie_id]\n",
    "        user_bias = algo.bu[internal_user_id]\n",
    "        \n",
    "        return user_bias, item_bias\n",
    "\n",
    "    except ValueError:\n",
    "        # This happens if the user or movie ID was not in the training set (a cold start case)\n",
    "        print(f\"User {test_user_id} or Movie {test_movie_id} was not in the training set (Cold Start).\")\n",
    "        return None, None\n",
    "\n",
    "prediction = algo.predict(test_user_id, test_movie_id)\n",
    "predicted_rating = prediction.est\n",
    "\n",
    "# Get the learned bias terms using the new helper function\n",
    "user_bias, item_bias = get_learned_biases(algo, test_user_id, test_movie_id)\n",
    "model_mu = algo.default_prediction() \n",
    "\n",
    "print(f\"\\n--- 6. Example Prediction for User {test_user_id} on Movie {test_movie_id} ---\")\n",
    "print(f\"Predicted rating: {predicted_rating:.4f}\")\n",
    "\n",
    "# --- 7. Display Learned Biases ---\n",
    "\n",
    "if user_bias is not None:\n",
    "    # We use the prediction.details dictionary to retrieve the interaction term \n",
    "    # (which is the residual error after accounting for biases)\n",
    "    interaction_term = predicted_rating - (model_mu + user_bias + item_bias)\n",
    "\n",
    "    print(\"\\n--- 7. Learned Biases from SVD ---\")\n",
    "    print(f\"Learned Global Mean (\\u03bc): {model_mu:.4f}\")\n",
    "    print(f\"Learned User Bias (b_u):   {user_bias:.4f}\")\n",
    "    print(f\"Learned Item Bias (b_i):   {item_bias:.4f}\")\n",
    "    print(f\"Learned Interaction Term:  {interaction_term:.4f}\")\n",
    "    print(f\"\\nPrediction Check: \\u03bc + b_u + b_i + interaction_term = {model_mu + user_bias + item_bias + interaction_term:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22ae036a-ab1c-4a08-9209-24d6ed4fcb35",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_baseline_rmse(trainset, testset):\n",
    "    \"\"\"Trains the BaselineOnly model and calculates its RMSE on the test set.\"\"\"\n",
    "    print(\"\\n--- 4A. Baseline Model (mu + b_u + b_i) ---\")\n",
    "    bsl_options = {'method': 'sgd'} # Use Stochastic Gradient Descent for optimization\n",
    "    \n",
    "    # Instantiate the BaselineOnly model\n",
    "    algo_baseline = BaselineOnly(bsl_options=bsl_options)\n",
    "    \n",
    "    # Train and predict\n",
    "    algo_baseline.fit(trainset)\n",
    "    predictions_baseline = algo_baseline.test(testset)\n",
    "    \n",
    "    rmse_baseline = accuracy.rmse(predictions_baseline, verbose=False)\n",
    "    mae_baseline = accuracy.mae(predictions_baseline, verbose=False)\n",
    "    \n",
    "    print(f\"Baseline RMSE: {rmse_baseline:.4f}\")\n",
    "    print(f\"Baseline MAE: {mae_baseline:.4f}\")\n",
    "    \n",
    "    return rmse_baseline, mae_baseline, predictions_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "922150ff-5e06-4732-9e1a-edeee6a30a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 4A. Baseline Model (mu + b_u + b_i) ---\n",
      "Estimating biases using sgd...\n",
      "Baseline RMSE: 0.8641\n",
      "Baseline MAE: 0.6593\n"
     ]
    }
   ],
   "source": [
    "rmse_baseline, mae_baseline, predictions_baseline = calculate_baseline_rmse(trainset, testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3855916-9dc2-4105-88bf-17dd8322dede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=======================================================\n",
      "| Performance Improvement: 0.0868 RMSE reduction |\n",
      "| SVD is BETTER than Baseline |\n",
      "=======================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=======================================================\")\n",
    "print(f\"| Performance Improvement: {rmse_baseline - rmse:.4f} RMSE reduction |\")\n",
    "print(f\"| SVD is {'BETTER' if rmse < rmse_baseline else 'WORSE'} than Baseline |\")\n",
    "print(\"=======================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38851881-cd6a-4297-a488-2d2159c84fc2",
   "metadata": {},
   "source": [
    "### Changing evaluation metric from accuracy to Precision@k and recall@k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9fc754b-4f2f-4ac9-ae72-cb8ef606a09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n(predictions, n=10, threshold=4.0):\n",
    "    \"\"\"\n",
    "    Returns the top N recommendations for each user, based on predicted rating (est).\n",
    "    \n",
    "    A movie is considered \"relevant\" if the ACTUAL rating (r_ui) is above the threshold.\n",
    "    \"\"\"\n",
    "    # 1. Map the predictions to each user\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, r_ui, est, _ in predictions:\n",
    "        top_n[uid].append((iid, r_ui, est)) # Store item ID, actual rating, predicted rating\n",
    "\n",
    "    # 2. Sort the predictions for each user and retrieve the k highest ones\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        # Sort by predicted rating (est) in descending order\n",
    "        user_ratings.sort(key=lambda x: x[2], reverse=True)\n",
    "        # Keep only the top N items\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "\n",
    "    return top_n\n",
    "\n",
    "def precision_recall_at_k(predictions, k=10, threshold=4.0):\n",
    "    \"\"\"\n",
    "    Calculates Precision@k and Recall@k for the predictions.\n",
    "    Relevance is defined by the actual rating (r_ui) being >= threshold.\n",
    "    \"\"\"\n",
    "    user_to_items = defaultdict(list)\n",
    "    \n",
    "    # 1. Map all relevant items (Actual Ratings >= Threshold) per user\n",
    "    for uid, iid, r_ui, est, _ in predictions:\n",
    "        if r_ui >= threshold:\n",
    "            user_to_items[uid].append(iid) # The list of items the user ACTUALLY liked (Relevant Items)\n",
    "\n",
    "    # 2. Get the top N predictions based on the *predicted score*\n",
    "    top_n = get_top_n(predictions, n=k, threshold=threshold)\n",
    "\n",
    "    precisions = dict()\n",
    "    recalled_items = dict()\n",
    "    \n",
    "    # 3. Calculate Precision and Recall for each user\n",
    "    for uid, recommended_items in top_n.items():\n",
    "        # Recommended items are tuples: (iid, r_ui, est)\n",
    "        \n",
    "        # True Positives: Recommended items that were actually relevant (r_ui >= threshold)\n",
    "        n_relevant_and_recommended = sum(1 for (iid, r_ui, est) in recommended_items if r_ui >= threshold)\n",
    "        \n",
    "        # Denominator for Recall: Total number of relevant items in the Test Set\n",
    "        n_relevant_total = len(user_to_items[uid])\n",
    "        \n",
    "        # Precision@k: (Relevant and Recommended) / (Total Recommended @ k)\n",
    "        precisions[uid] = n_relevant_and_recommended / k if k > 0 else 0\n",
    "        \n",
    "        # Recall@k: (Relevant and Recommended) / (Total Relevant in Test Set)\n",
    "        recalled_items[uid] = n_relevant_and_recommended / n_relevant_total if n_relevant_total > 0 else 0\n",
    "\n",
    "    # Return the average across all users\n",
    "    return np.mean(list(precisions.values())), np.mean(list(recalled_items.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "803b7f4c-2a42-4ff8-a5e3-a0fb7544a4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_VAL = 10\n",
    "THRESHOLD = 4.0 # Define a rating threshold for 'relevance' (e.g., ratings >= 4.0 are considered a 'like')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c0a2380-52a6-4acd-a70c-d4997da3f4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 4. Ranking Evaluation (P@k, R@k) ---\n",
      "Threshold for Relevance: Actual Rating >= 4.0\n",
      "K (List Size): 10\n",
      "-----------------------------------\n",
      "| Model | Precision@10 | Recall@10 |\n",
      "-----------------------------------\n",
      "| SVD   | 0.6027      | 0.6965     |\n",
      "| Baseline | 0.5694   | 0.6773     |\n",
      "-----------------------------------\n",
      "\n",
      "✅ SVD performed better on Precision@k, meaning its top 10 recommendations are more relevant.\n"
     ]
    }
   ],
   "source": [
    "# Calculate P@k and R@k for SVD\n",
    "precision_svd, recall_svd = precision_recall_at_k(predictions, k=K_VAL, threshold=THRESHOLD)\n",
    "# Calculate P@k and R@k for Baseline\n",
    "precision_baseline, recall_baseline = precision_recall_at_k(predictions_baseline, k=K_VAL, threshold=THRESHOLD)\n",
    "\n",
    "print(\"\\n--- 4. Ranking Evaluation (P@k, R@k) ---\")\n",
    "print(f\"Threshold for Relevance: Actual Rating >= {THRESHOLD}\")\n",
    "print(f\"K (List Size): {K_VAL}\")\n",
    "print(\"-\" * 35)\n",
    "print(f\"| Model | Precision@{K_VAL} | Recall@{K_VAL} |\")\n",
    "print(\"-\" * 35)\n",
    "print(f\"| SVD   | {precision_svd:.4f}      | {recall_svd:.4f}     |\")\n",
    "print(f\"| Baseline | {precision_baseline:.4f}   | {recall_baseline:.4f}     |\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "if precision_svd > precision_baseline:\n",
    "    print(\"\\n✅ SVD performed better on Precision@k, meaning its top 10 recommendations are more relevant.\")\n",
    "else:\n",
    "    print(\"\\n⚠️ Baseline performed better or equal on Precision@k. SVD needs tuning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f6839cc-5643-4d50-98f2-792ac9887776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1147880044</td>\n",
       "      <td>Pulp Fiction (1994)</td>\n",
       "      <td>Comedy|Crime|Drama|Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>306</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1147868817</td>\n",
       "      <td>Three Colors: Red (Trois couleurs: Rouge) (1994)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>307</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1147868828</td>\n",
       "      <td>Three Colors: Blue (Trois couleurs: Bleu) (1993)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>665</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1147878820</td>\n",
       "      <td>Underground (1995)</td>\n",
       "      <td>Comedy|Drama|War</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>899</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1147868510</td>\n",
       "      <td>Singin' in the Rain (1952)</td>\n",
       "      <td>Comedy|Musical|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25000090</th>\n",
       "      <td>162541</td>\n",
       "      <td>50872</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1240953372</td>\n",
       "      <td>Ratatouille (2007)</td>\n",
       "      <td>Animation|Children|Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25000091</th>\n",
       "      <td>162541</td>\n",
       "      <td>55768</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1240951998</td>\n",
       "      <td>Bee Movie (2007)</td>\n",
       "      <td>Animation|Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25000092</th>\n",
       "      <td>162541</td>\n",
       "      <td>56176</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1240950697</td>\n",
       "      <td>Alvin and the Chipmunks (2007)</td>\n",
       "      <td>Children|Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25000093</th>\n",
       "      <td>162541</td>\n",
       "      <td>58559</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1240953434</td>\n",
       "      <td>Dark Knight, The (2008)</td>\n",
       "      <td>Action|Crime|Drama|IMAX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25000094</th>\n",
       "      <td>162541</td>\n",
       "      <td>63876</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1240952515</td>\n",
       "      <td>Milk (2008)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000095 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          userId  movieId  rating   timestamp  \\\n",
       "0              1      296     5.0  1147880044   \n",
       "1              1      306     3.5  1147868817   \n",
       "2              1      307     5.0  1147868828   \n",
       "3              1      665     5.0  1147878820   \n",
       "4              1      899     3.5  1147868510   \n",
       "...          ...      ...     ...         ...   \n",
       "25000090  162541    50872     4.5  1240953372   \n",
       "25000091  162541    55768     2.5  1240951998   \n",
       "25000092  162541    56176     2.0  1240950697   \n",
       "25000093  162541    58559     4.0  1240953434   \n",
       "25000094  162541    63876     5.0  1240952515   \n",
       "\n",
       "                                                     title  \\\n",
       "0                                      Pulp Fiction (1994)   \n",
       "1         Three Colors: Red (Trois couleurs: Rouge) (1994)   \n",
       "2         Three Colors: Blue (Trois couleurs: Bleu) (1993)   \n",
       "3                                       Underground (1995)   \n",
       "4                               Singin' in the Rain (1952)   \n",
       "...                                                    ...   \n",
       "25000090                                Ratatouille (2007)   \n",
       "25000091                                  Bee Movie (2007)   \n",
       "25000092                    Alvin and the Chipmunks (2007)   \n",
       "25000093                           Dark Knight, The (2008)   \n",
       "25000094                                       Milk (2008)   \n",
       "\n",
       "                               genres  \n",
       "0         Comedy|Crime|Drama|Thriller  \n",
       "1                               Drama  \n",
       "2                               Drama  \n",
       "3                    Comedy|Drama|War  \n",
       "4              Comedy|Musical|Romance  \n",
       "...                               ...  \n",
       "25000090     Animation|Children|Drama  \n",
       "25000091             Animation|Comedy  \n",
       "25000092              Children|Comedy  \n",
       "25000093      Action|Crime|Drama|IMAX  \n",
       "25000094                        Drama  \n",
       "\n",
       "[25000095 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "merged_data_df = pd.merge(ratings, movies, on='movieId', how='left')\n",
    "display(merged_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "42885948-c261-423f-8dc4-2bb0c6bc844a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STEP 1: Calculating Genre-Based Content Score ---\n",
      "Calculated average rating for 20 genres.\n",
      "Top 5 Genres by Average Rating:\n",
      "          genres  genre_avg_rating\n",
      "10    Film-Noir          3.925728\n",
      "18          War          3.791466\n",
      "7   Documentary          3.705281\n",
      "6         Crime          3.685044\n",
      "8         Drama          3.677185\n",
      "\n",
      "Movie Feature (G_score) created:\n",
      "                                title  \\\n",
      "0                    Toy Story (1995)   \n",
      "1                      Jumanji (1995)   \n",
      "2             Grumpier Old Men (1995)   \n",
      "3            Waiting to Exhale (1995)   \n",
      "4  Father of the Bride Part II (1995)   \n",
      "5                         Heat (1995)   \n",
      "6                      Sabrina (1995)   \n",
      "7                 Tom and Huck (1995)   \n",
      "8                 Sudden Death (1995)   \n",
      "9                    GoldenEye (1995)   \n",
      "\n",
      "                                        genres   G_score  \n",
      "0  Adventure|Animation|Children|Comedy|Fantasy  3.500096  \n",
      "1                   Adventure|Children|Fantasy  3.487180  \n",
      "2                               Comedy|Romance  3.483352  \n",
      "3                         Comedy|Drama|Romance  3.547963  \n",
      "4                                       Comedy  3.423992  \n",
      "5                        Action|Crime|Thriller  3.558200  \n",
      "6                               Comedy|Romance  3.483352  \n",
      "7                           Adventure|Children  3.474976  \n",
      "8                                       Action  3.466592  \n",
      "9                    Action|Adventure|Thriller  3.502333  \n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- STEP 1: Calculating Genre-Based Content Score ---\")\n",
    "\n",
    "# 1.1 Explode the ratings data to link every rating to every genre\n",
    "ratings_exploded = merged_data_df[['movieId', 'rating', 'genres']].copy()\n",
    "ratings_exploded['genres'] = ratings_exploded['genres'].str.split('|')\n",
    "ratings_exploded = ratings_exploded.explode('genres')\n",
    "\n",
    "# 1.2 Calculate the global mean rating for each genre\n",
    "genre_mean_ratings = ratings_exploded.groupby('genres')['rating'].mean().reset_index(name='genre_avg_rating')\n",
    "print(f\"Calculated average rating for {len(genre_mean_ratings)} genres.\")\n",
    "print(\"Top 5 Genres by Average Rating:\\n\", genre_mean_ratings.sort_values('genre_avg_rating', ascending=False).head(5))\n",
    "\n",
    "# 1.3 Create a function to calculate a movie's genre score\n",
    "genre_dict = genre_mean_ratings.set_index('genres')['genre_avg_rating'].to_dict()\n",
    "\n",
    "def calculate_movie_genre_score(genres_str):\n",
    "    \"\"\"Calculates the mean of the average ratings of all a movie's genres.\"\"\"\n",
    "    if pd.isna(genres_str) or genres_str == '(no genres listed)':\n",
    "        return np.nan\n",
    "    \n",
    "    genres = genres_str.split('|')\n",
    "    scores = [genre_dict.get(g, np.nan) for g in genres]\n",
    "    \n",
    "    # Return the mean of non-NaN genre scores\n",
    "    return np.mean([s for s in scores if not np.isnan(s)])\n",
    "\n",
    "# Apply the function to all movies (use movies_df for unique movies)\n",
    "movies['G_score'] = movies['genres'].apply(calculate_movie_genre_score)\n",
    "\n",
    "print(\"\\nMovie Feature (G_score) created:\")\n",
    "print(movies[['title', 'genres', 'G_score']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9ccb704-7e8c-4817-bdca-939ab6592ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP 2: The Hybrid Prediction Logic ---\n",
    "\n",
    "def hybrid_cold_start_prediction(user_id, movie_id, svd_model, min_ratings_threshold=50):\n",
    "    \"\"\"\n",
    "    Predicts a rating using SVD if warm, or the Genre Score if cold.\n",
    "    \n",
    "    NOTE: This function needs access to 'merged_data_df', 'movies_df', and the trained 'svd_model'.\n",
    "    \"\"\"\n",
    "    # 1. Determine User Status (Cold/Warm) and get b_u\n",
    "    is_user_cold = False\n",
    "    try:\n",
    "        internal_user_id = svd_model.trainset.to_inner_uid(user_id)\n",
    "        user_bias = svd_model.bu[internal_user_id] \n",
    "    except ValueError:\n",
    "        is_user_cold = True\n",
    "        user_bias = 0.0 # New users have zero learned user bias initially\n",
    "\n",
    "    # 2. Determine Item Status (Cold/Warm)\n",
    "    movie_stats = merged_data_df.groupby('movieId')['rating'].count()\n",
    "    rating_count = movie_stats.get(movie_id, 0)\n",
    "    is_item_cold = rating_count < min_ratings_threshold\n",
    "    \n",
    "    mu = svd_model.default_prediction()\n",
    "    prediction = 0.0\n",
    "    source = \"\"\n",
    "\n",
    "    # --- SCENARIO A: WARM USER, WARM ITEM -> Full Personalization ---\n",
    "    if not is_user_cold and not is_item_cold:\n",
    "        prediction = svd_model.predict(user_id, movie_id).est\n",
    "        source = \"SVD (Warm)\"\n",
    "        \n",
    "    # --- SCENARIO B: NEW USER, WARM ITEM -> Popularity Baseline (mu + b_i) ---\n",
    "    elif is_user_cold and not is_item_cold:\n",
    "        try:\n",
    "            # Use learned item bias from SVD for the popularity baseline\n",
    "            internal_movie_id = svd_model.trainset.to_inner_iid(movie_id)\n",
    "            item_bias = svd_model.bi[internal_movie_id]\n",
    "            \n",
    "            prediction = mu + item_bias # User bias is 0.0\n",
    "            source = \"Popularity Baseline (New User: mu + b_i)\"\n",
    "        except ValueError:\n",
    "            # Should not happen if item is warm, but defensive coding\n",
    "            prediction = mu \n",
    "            source = \"Global Mean (New User, Error fetching b_i)\"\n",
    "            \n",
    "    # --- SCENARIO C: WARM USER, NEW ITEM -> Personalized Content Fallback (mu + b_u + G-Score) ---\n",
    "    elif not is_user_cold and is_item_cold:\n",
    "        genre_score = movies_df[movies_df['movieId'] == movie_id]['G_score'].iloc[0]\n",
    "        # The item bias is derived from the genre score relative to the global mean\n",
    "        item_content_bias = genre_score - mu\n",
    "        \n",
    "        prediction = mu + user_bias + item_content_bias\n",
    "        source = \"Personalized Content (New Item: mu + b_u + G_score_Bias)\"\n",
    "        \n",
    "    # --- SCENARIO D: NEW USER, NEW ITEM -> Pure Content Fallback (mu + G-Score) ---\n",
    "    elif is_user_cold and is_item_cold:\n",
    "        genre_score = movies_df[movies_df['movieId'] == movie_id]['G_score'].iloc[0]\n",
    "        # The item bias is derived from the genre score relative to the global mean\n",
    "        item_content_bias = genre_score - mu\n",
    "        \n",
    "        prediction = mu + 0.0 + item_content_bias # User bias is 0.0\n",
    "        source = \"Popularity-Content Fallback (Both Cold: mu + G_score_Bias)\"\n",
    "\n",
    "    # Cap the final prediction score\n",
    "    prediction = max(0.5, min(5.0, prediction))\n",
    "\n",
    "    return prediction, source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb19d06c-f1df-459b-bbef-a964760ba38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_USER_ID= 162541\n",
    "COLD_MOVIE_ID = 296\n",
    "\n",
    "# TEST_USER_ID= 162541\n",
    "# COLD_MOVIE_ID = 899"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84ecbdc5-b502-425b-bb5c-8054691f49f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- DEMO: Hybrid Prediction for Cold Start Movie 296 ---\n",
      "Final Prediction: 4.8430 (Source: SVD (Warm))\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n--- DEMO: Hybrid Prediction for Cold Start Movie {COLD_MOVIE_ID} ---\")\n",
    "predicted_score, source = hybrid_cold_start_prediction(\n",
    "    user_id=TEST_USER_ID, \n",
    "    movie_id=COLD_MOVIE_ID, \n",
    "    svd_model=algo \n",
    ")\n",
    "print(f\"Final Prediction: {predicted_score:.4f} (Source: {source})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d15fa2aa-0cd6-4c70-a153-ac92dc202c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cold_testset(testset, trainset):\n",
    "    \"\"\"\n",
    "    Filters the testset to include only ratings where either user OR item \n",
    "    was NOT present in the training set (Cold-Only Evaluation).\n",
    "    Returns the cold set and the warm set (for verification).\n",
    "    \"\"\"\n",
    "    warm_testset = []\n",
    "    cold_testset = []\n",
    "    \n",
    "    train_users = set(trainset.ur.keys())\n",
    "    train_items = set(trainset.ir.keys())\n",
    "    \n",
    "    for uid, iid, r_ui in testset:\n",
    "        try:\n",
    "            inner_uid = trainset.to_inner_uid(uid)\n",
    "            inner_iid = trainset.to_inner_iid(iid)\n",
    "            \n",
    "            is_warm = (inner_uid in train_users) and (inner_iid in train_items)\n",
    "            \n",
    "            if is_warm:\n",
    "                warm_testset.append((uid, iid, r_ui))\n",
    "            else:\n",
    "                cold_testset.append((uid, iid, r_ui))\n",
    "        except ValueError:\n",
    "            # Item/User ID was not in the original dataset's universe (very cold)\n",
    "            cold_testset.append((uid, iid, r_ui))\n",
    "\n",
    "    print(f\"Total test ratings: {len(testset)}\")\n",
    "    print(f\"Warm-Only ratings: {len(warm_testset)}\")\n",
    "    print(f\"Cold-Only ratings: {len(cold_testset)}\")\n",
    "    \n",
    "    return cold_testset, warm_testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b255beea-a456-410f-9ee5-3bce2a783be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cold_predictions(cold_testset, svd_model, movies_df, global_mean):\n",
    "    \"\"\"\n",
    "    Generates predictions on the cold test set using two strategies:\n",
    "    1. SVD Default: Predicts the Global Mean (mu) - Represents SVD failure.\n",
    "    2. Hybrid Fallback: Uses the mu + b_i/G_score logic - Represents the solution.\n",
    "    \"\"\"\n",
    "    predictions_svd_default = []\n",
    "    predictions_hybrid_fallback = []\n",
    "    \n",
    "    for uid, iid, r_ui in cold_testset:\n",
    "        # --- 1. SVD DEFAULT (Pure Global Mean) ---\n",
    "        # SVD model always defaults to mu when user/item factors are missing/zero\n",
    "        svd_default_est = global_mean \n",
    "        details = (uid, iid, r_ui, svd_default_est, {})\n",
    "        predictions_svd_default.append(details)\n",
    "        \n",
    "        # --- 2. CUSTOM HYBRID FALLBACK ---\n",
    "        \n",
    "        # Replicate the logic from the hybrid_cold_start_prediction function\n",
    "        user_bias = 0.0\n",
    "        try:\n",
    "            # Check for Warm User Bias (Scenario C)\n",
    "            internal_user_id = svd_model.trainset.to_inner_uid(uid)\n",
    "            user_bias = svd_model.bu[internal_user_id] \n",
    "        except ValueError:\n",
    "            # User is Cold (Scenario B or D)\n",
    "            user_bias = 0.0 \n",
    "        \n",
    "        # Get G-Score for Item Bias\n",
    "        movie_row = movies_df[movies_df['movieId'] == iid]\n",
    "        if not movie_row.empty and 'G_score' in movie_row.columns:\n",
    "            genre_score = movie_row['G_score'].iloc[0]\n",
    "            if np.isnan(genre_score):\n",
    "                # Fallback if genre data is missing even in the movie list (rare)\n",
    "                item_bias_term = 0.0\n",
    "            else:\n",
    "                item_bias_term = genre_score - global_mean\n",
    "        else:\n",
    "            # If no G_score is available, revert to simple mu (mu + 0)\n",
    "            item_bias_term = 0.0\n",
    "\n",
    "        # Hybrid Prediction: mu + b_u + Item_Bias_Term (derived from G-Score)\n",
    "        hybrid_est = global_mean + user_bias + item_bias_term\n",
    "        \n",
    "        # Cap the score\n",
    "        hybrid_est = max(0.5, min(5.0, hybrid_est))\n",
    "\n",
    "        details = (uid, iid, r_ui, hybrid_est, {})\n",
    "        predictions_hybrid_fallback.append(details)\n",
    "\n",
    "    return predictions_svd_default, predictions_hybrid_fallback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25893e2-d738-490a-8c38-d70f921a45bd",
   "metadata": {},
   "source": [
    "# Check if hybrid fallback is effective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff04393b-2757-4e18-acbc-16aaa3b21d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total test ratings: 5000019\n",
      "Warm-Only ratings: 4997258\n",
      "Cold-Only ratings: 2761\n",
      "Estimating biases using sgd...\n",
      "\n",
      "--- 3. WARM SET (Standard SVD vs. Baseline) ---\n",
      "Warm SVD RMSE: 0.7772 | Warm Baseline RMSE: 0.8641\n",
      "\n",
      "--- 4. COLD SET (SVD Failure vs. Hybrid Solution) ---\n",
      "This comparison proves the value of the custom fallback logic:\n",
      "| SVD Default (RMSE on Cold Set): 1.2685\n",
      "| Hybrid Fallback (RMSE on Cold Set): 0.9532\n",
      "\n",
      "✅ **Conclusion:** The Hybrid Fallback reduces the Cold Set RMSE by 0.3152, demonstrating robust problem-solving.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 1. Filter the Test Set into Cold and Warm ---\n",
    "cold_testset, warm_testset = get_cold_testset(testset, trainset)\n",
    "\n",
    "# --- 3. WARM SET Evaluation (Standard Benchmark) ---\n",
    "predictions_svd_warm = algo.test(warm_testset)\n",
    "predictions_baseline_warm = BaselineOnly(bsl_options={'method': 'sgd'}).fit(trainset).test(warm_testset)\n",
    "rmse_svd_warm = accuracy.rmse(predictions_svd_warm, verbose=False)\n",
    "rmse_baseline_warm = accuracy.rmse(predictions_baseline_warm, verbose=False)\n",
    "\n",
    "print(\"\\n--- 3. WARM SET (Standard SVD vs. Baseline) ---\")\n",
    "print(f\"Warm SVD RMSE: {rmse_svd_warm:.4f} | Warm Baseline RMSE: {rmse_baseline_warm:.4f}\")\n",
    "\n",
    "# --- 4. COLD SET Evaluation (Hybrid Fallback vs. SVD Failure) ---\n",
    "svd_default_preds, hybrid_fallback_preds = get_cold_predictions(\n",
    "    cold_testset, \n",
    "    algo, \n",
    "    movies, \n",
    "    global_mean\n",
    ")\n",
    "\n",
    "# RMSE for the SVD failure (prediction = mu for all cold ratings)\n",
    "rmse_svd_default = accuracy.rmse(svd_default_preds, verbose=False) \n",
    "# RMSE for the custom Hybrid Fallback logic (prediction = mu + b_u + G_score_Bias)\n",
    "rmse_hybrid_fallback = accuracy.rmse(hybrid_fallback_preds, verbose=False)\n",
    "\n",
    "print(\"\\n--- 4. COLD SET (SVD Failure vs. Hybrid Solution) ---\")\n",
    "print(\"This comparison proves the value of the custom fallback logic:\")\n",
    "print(f\"| SVD Default (RMSE on Cold Set): {rmse_svd_default:.4f}\")\n",
    "print(f\"| Hybrid Fallback (RMSE on Cold Set): {rmse_hybrid_fallback:.4f}\")\n",
    "\n",
    "if rmse_hybrid_fallback < rmse_svd_default:\n",
    "    print(f\"\\n✅ **Conclusion:** The Hybrid Fallback reduces the Cold Set RMSE by {rmse_svd_default - rmse_hybrid_fallback:.4f}, demonstrating robust problem-solving.\")\n",
    "else:\n",
    "     print(\"\\n⚠️ **Conclusion:** The Hybrid Fallback did not beat the SVD default.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40b3267-3208-45d1-a87b-5cfbd94e6d1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
